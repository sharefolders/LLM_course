**Задание 1. Дообучение декодерной модели - 5 баллов**

1. **Выбор датасета** - **0.5 балла**
   - Выберите и загрузите датасет для задачи генерации текста 
   - Подходящие варианты: 
     * Текстовые корпусы на платформе Hugging Face
     * Ваши собственные данные
     
2. **Выбор предобученной модели - 0.5 балла**
   - Выберите подходящую (по размеру - от 1B параметров) предобученную языковую модель на Hugging Face Model Hub

3. **Предварительная оценка качества - 1 балл**
   - Соберите небольшую "корзинку" тестовых примеров, прогоните их через модель для оценки качества генерации перед дообучением
   - Дополнительно прогоните модель через выбранную вами одну или несколько релевантных задач из lm-evaluation-harness

4. **QLora-дообучение - 2 балла**
   - Настройте параметры дообучения через QLora, опишите свой выбор значений и настраиваемых параметров в комментариях
   - Обучите модель на вашем датасете

5. **Оценка качества обучения - - 1 балл**
   - Проверьте качество генерации на бенчмарке и "корзинке" после дообучения
   
   
**Задание 2. Дообучение энкодерной модели - 5 баллов**

1. **Выбор датасета - 0.5 балла**
   - Выберите и загрузите датасет для контрастивного дообучения
   - Подходящие варианты: 
     * QA-датасеты
     * Ваши собственные данные - главное, чтобы в них были пары anchor+positive
     
2. **Выбор предобученной модели - 0.5 балла**
   - Выберите энкодерную модель для дообучения 

3. **Предварительная оценка качества - 1 балл**
   - Выделите валидационное множество, проверьте на нем качество поиска с помощью метрик @k. Также для валидации можно использовать down-stream задачу на эмбеддингах из модели, если она у вас есть
   
4. **QLora-дообучение - 2 балла**
   - Настройте параметры дообучения через QLora, опишите свой выбор значений и настраиваемых параметров в комментариях
   - Обучите модель на вашем датасете

5. **Оценка качества обучения  - 1 балл**
   - Проверьте качество генерации на валидационном датасете


**Задание 3. Добавление токенов в токенизатор - дополнительное задание для тех, кто хочет попробовать**

1. **Выбор датасета**
   - Выберите и загрузите датасет для задачи генерации текста 
   - Подходящие варианты: 
     * Текстовые корпусы на платформе Hugging Face
     * Ваши собственные данные
     
2. **Выбор предобученной модели**
   - Выберите подходящую, на ваш взгляд, предобученную языковую модель на Hugging Face Model Hub

3. **Выбор терминологии**
   - Найдите слова или аббревиатуры, которых нет в словаре вашей модели (например, «айтишка», «крипта», специфический термини т.п.).

4. **Демонстрация "до"**
   - Покажите, как токенизатор разбивает эти слова по умолчанию. Выведите список токенов и их ID.

5. **Модификация токенизатора и модели**
   - Добавьте эти слова в словарь токенизатора как один цельный токен.
   - Измените размерность матрицы эмбеддингов и выходного классификационного слоя модели.
   - Инициализируйте эмбеддинги новых токены усреднением старых (или используйте другую инициализацию, если уверены в ней).
   
6. **Дообучение модели**
   - Создайте / подыщите датасет с достаточным количество примеров употребления новых токенов (например, 50-100 текстов на токен).
   - Дообучите модель на датасете в несколько этапов:
      - только эмбеддинги токенов
      - матрица эмбеддингов + классификатор
      - вся модель (можно попробовать дообучить с lora на attention / attention + MLP)
      
7. Оцените качестве дообученных эмбеддингов через внутренние метрики (например, cosine distance до якорных токенов) или при генерации