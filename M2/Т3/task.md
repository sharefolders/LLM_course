### 1. Выбор предобученной модели

* Выберите понравившуюся вам предобученную языковую модель на Hugging Face Model Hub (по размеру лучше поменьше — до 0.5–1B параметров)

### 2. Базовый прогон — 2 балла

* Проведите прогоны модели для длины входа $L\in\{\approx16, \approx32, \approx128\}$, `batch_size = 1`, max_new_tokens $\approx 64$. Замерьте потребление VRAM, TTFT, TPS, E2E (prefill + decode time) для каждого L и кратко интерпретируйте тренды (prefill vs decode).

### 3. Профайлинг модели через pytorch profiler + tensorboard — 4 балла

* Запустите PyTorch Profiler для CPU + CUDA: `record_shapes`, `profile_memory`, `with_stack`
* Снимите два сценария:
    
    * prefill-доминантный (длинный вход , короткая генерация)
    * decode-доминантный (вход , генерация )

* Визуализируйте результаты прогонов в tensorboard
* Проинтерпретируйте полученные результаты:
    * Выделите top-5 операций по CUDA-времени и их доли: attention, GEMM / matmul, копирования HtoD / DtoH и др.
    * Постарайтесь найти признаки compute-bound и memory-bound сценариев, а также оверхеда на коммуникации на уровне операций / kernels
    * Сопоставьте prefill vs decode: где сосредоточено время, есть ли узкие места (например, много мелких kernel launch на decode)
    * Сделайте вывод, какие оптимизации из рассмотренных на лекции релевантны найденным узким местам

### 4. Эксперименты с квантизованной моделью — 4 балла

* Загрузите ту же модель в 4-bit (bitsandbytes)
* Сравните потребление VRAM FP16 vs 4-bit
* Повторите эксперимент с варьированием длины входа L и замерами метрик для prefill / decode (как в пункте 2) и интерпретируйте результаты
* Запустите pytorch profiler для квантизованной модели, визуализируйте и интерпретируйте результаты (как в пункте 3). Отметьте изменения по времени / метрикам / операциям / трейсах по сравнению с прогонами FP16-модели (если есть).