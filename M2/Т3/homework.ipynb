{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "559b2288",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch.profiler as profiler\n",
    "from torch.profiler import ProfilerActivity, profile, record_function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9e10313",
   "metadata": {},
   "source": [
    "# Задание 1: Выбор предобученной модели"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1c2d3e4",
   "metadata": {},
   "source": [
    "### 1.1. Выбор и загрузка модели\n",
    "\n",
    "Для выполнения задания выберем модель `sberbank-ai/rugpt3small_based_on_gpt2`. \n",
    "\n",
    "Это русскоязычная GPT-модель с 125 миллионами параметров, что соответствует условию (до 1B параметров) и хорошо подходит для экспериментов с профилированием на ограниченных ресурсах."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f5g6h7i8-fixed-loading",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Загружаю модель из локальной копии: ./rugpt3small_local...\n",
      "Модель и токенизатор успешно загружены на cuda.\n",
      "Количество параметров: 125.2M\n"
     ]
    }
   ],
   "source": [
    "MODEL_NAME = \"sberbank-ai/rugpt3small_based_on_gpt2\"\n",
    "LOCAL_MODEL_PATH = \"./rugpt3small_local\"\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "\n",
    "def load_or_get_model(model_name, local_path, device):\n",
    "    if \"model\" in globals() and \"tokenizer\" in globals():\n",
    "        print(\"Модель уже в памяти, переиспользую без перезагрузки.\")\n",
    "        model.to(device)\n",
    "        return tokenizer, model\n",
    "\n",
    "    if os.path.isdir(local_path):\n",
    "        print(f\"Загружаю модель из локальной копии: {local_path}...\")\n",
    "        tokenizer = AutoTokenizer.from_pretrained(local_path, local_files_only=True)\n",
    "        model = AutoModelForCausalLM.from_pretrained(local_path, local_files_only=True)\n",
    "    else:\n",
    "        print(f\"Локальная копия модели не найдена. Скачиваю {model_name}...\")\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "        print(f\"Сохраняю модель в {local_path}...\")\n",
    "        model.save_pretrained(local_path)\n",
    "        tokenizer.save_pretrained(local_path)\n",
    "        print(\"Модель успешно сохранена.\")\n",
    "\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    model.config.use_cache = True\n",
    "\n",
    "    if tokenizer.pad_token_id is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "        model.config.pad_token_id = model.config.eos_token_id\n",
    "\n",
    "    print(f\"Модель и токенизатор успешно загружены на {device}.\")\n",
    "    print(f\"Количество параметров: {model.num_parameters() / 1e6:.1f}M\")\n",
    "\n",
    "    return tokenizer, model\n",
    "\n",
    "\n",
    "try:\n",
    "    tokenizer, model = load_or_get_model(MODEL_NAME, LOCAL_MODEL_PATH, DEVICE)\n",
    "except Exception as e:\n",
    "    print(f\"Произошла ошибка: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "task2-header-final-correct",
   "metadata": {},
   "source": [
    "# Задание 2: Базовый прогон (финальная, корректная версия)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "task2-measurement-final-correct",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Прогрев GPU...\n",
      "Прогрев завершен.\n",
      "\n",
      "--- Измерения для L = 512 (усреднение по 10 итерациям) ---\n",
      "Результаты для L=512 сохранены.\n",
      "\n",
      "--- Измерения для L = 1024 (усреднение по 10 итерациям) ---\n",
      "Результаты для L=1024 сохранены.\n",
      "\n",
      "--- Измерения для L = 1536 (усреднение по 10 итерациям) ---\n",
      "Результаты для L=1536 сохранены.\n"
     ]
    }
   ],
   "source": [
    "@torch.inference_mode()\n",
    "def measure_performance(input_ids, max_new_tokens):\n",
    "    # --- Prefill ---\n",
    "    if DEVICE == \"cuda\":\n",
    "        torch.cuda.synchronize()\n",
    "    t0 = time.perf_counter()\n",
    "\n",
    "    # Прямой вызов модели для prefill'а, получаем KV-кэш\n",
    "    out = model(input_ids=input_ids, use_cache=True)\n",
    "\n",
    "    if DEVICE == \"cuda\":\n",
    "        torch.cuda.synchronize()\n",
    "    prefill_time = time.perf_counter() - t0\n",
    "\n",
    "    # --- Decode ---\n",
    "    past = out.past_key_values\n",
    "    # Берем последний токен для начала декодирования\n",
    "    next_token = out.logits[:, -1, :].argmax(dim=-1, keepdim=True)\n",
    "\n",
    "    decode_tokens = max(max_new_tokens - 1, 0)\n",
    "\n",
    "    if DEVICE == \"cuda\":\n",
    "        torch.cuda.synchronize()\n",
    "    t1 = time.perf_counter()\n",
    "\n",
    "    # Цикл декодирования\n",
    "    for _ in range(decode_tokens):\n",
    "        out = model(input_ids=next_token, past_key_values=past, use_cache=True)\n",
    "        past = out.past_key_values\n",
    "        next_token = out.logits[:, -1, :].argmax(dim=-1, keepdim=True)\n",
    "\n",
    "    if DEVICE == \"cuda\":\n",
    "        torch.cuda.synchronize()\n",
    "    decode_time = time.perf_counter() - t1\n",
    "    decode_time_per_token = decode_time / decode_tokens if decode_tokens > 0 else 0\n",
    "\n",
    "    # --- Расчет метрик ---\n",
    "    e2e_time = prefill_time + decode_time\n",
    "    tps = decode_tokens / decode_time if decode_time > 0 else 0\n",
    "\n",
    "    return {\n",
    "        \"TTFT (prefill, c)\": prefill_time,\n",
    "        \"Decode (c)\": decode_time,\n",
    "        \"Decode per token (c)\": decode_time_per_token,\n",
    "        \"E2E (c)\": e2e_time,\n",
    "        \"TPS (ток/с)\": tps,\n",
    "    }\n",
    "\n",
    "\n",
    "L_values = [512, 1024, 1536]\n",
    "max_new_tokens = 128\n",
    "num_iterations = 10\n",
    "all_results = []\n",
    "\n",
    "base_prompt = \"Искусственный интеллект и машинное обучение революционизировали современные технологии. \"\n",
    "base_tokens = tokenizer.encode(base_prompt)\n",
    "\n",
    "if DEVICE == \"cuda\":\n",
    "    print(\"Прогрев GPU...\")\n",
    "    warm_inputs = tokenizer(base_prompt, return_tensors=\"pt\").to(DEVICE)\n",
    "    _ = measure_performance(warm_inputs[\"input_ids\"], 16)\n",
    "    print(\"Прогрев завершен.\")\n",
    "\n",
    "for L in L_values:\n",
    "    print(f\"\\n--- Измерения для L = {L} (усреднение по {num_iterations} итерациям) ---\")\n",
    "    iter_results = []\n",
    "\n",
    "    repeat_count = max(1, L // max(1, len(base_tokens)))\n",
    "    prompt = base_prompt * repeat_count\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=L).to(DEVICE)\n",
    "    input_ids = inputs[\"input_ids\"]\n",
    "\n",
    "    for i in range(num_iterations):\n",
    "        if DEVICE == \"cuda\":\n",
    "            torch.cuda.reset_peak_memory_stats()\n",
    "            base_alloc = torch.cuda.memory_allocated()\n",
    "\n",
    "        perf_metrics = measure_performance(input_ids, max_new_tokens)\n",
    "\n",
    "        if DEVICE == \"cuda\":\n",
    "            peak_alloc = torch.cuda.max_memory_allocated()\n",
    "            # Пиковая память сверх базовой (весов модели), чтобы видеть вклад KV-кэша\n",
    "            perf_metrics[\"VRAM (MB)\"] = (peak_alloc - base_alloc) / (1024**2)\n",
    "        else:\n",
    "            perf_metrics[\"VRAM (MB)\"] = 0\n",
    "        iter_results.append(perf_metrics)\n",
    "\n",
    "    avg_results = pd.DataFrame(iter_results).mean().to_dict()\n",
    "    avg_results[\"L (вход)\"] = L\n",
    "    all_results.append(avg_results)\n",
    "    print(f\"Результаты для L={L} сохранены.\")\n",
    "\n",
    "# --- Отображение результатов ---\n",
    "df_results = pd.DataFrame(all_results)\n",
    "df_results = df_results[\n",
    "    [\"L (вход)\", \"VRAM (MB)\", \"TTFT (prefill, c)\", \"E2E (c)\", \"Decode (c)\", \"Decode per token (c)\", \"TPS (ток/с)\"]\n",
    "]\n",
    "df_results = df_results.round(4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "task2-display-final-correct",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "L (вход)",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "VRAM (MB)",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "TTFT (prefill, c)",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "E2E (c)",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Decode (c)",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Decode per token (c)",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "TPS (ток/с)",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "ref": "23df09fe-1f54-4378-bd14-dbb3fe0644e4",
       "rows": [
        [
         "0",
         "512",
         "128.2168",
         "0.0211",
         "1.3486",
         "1.3274",
         "0.0105",
         "96.1517"
        ],
        [
         "1",
         "1024",
         "238.2891",
         "0.0388",
         "1.2679",
         "1.2291",
         "0.0097",
         "103.7283"
        ],
        [
         "2",
         "1536",
         "358.3906",
         "0.0594",
         "1.299",
         "1.2396",
         "0.0098",
         "102.7924"
        ]
       ],
       "shape": {
        "columns": 7,
        "rows": 3
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>L (вход)</th>\n",
       "      <th>VRAM (MB)</th>\n",
       "      <th>TTFT (prefill, c)</th>\n",
       "      <th>E2E (c)</th>\n",
       "      <th>Decode (c)</th>\n",
       "      <th>Decode per token (c)</th>\n",
       "      <th>TPS (ток/с)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>512</td>\n",
       "      <td>128.2168</td>\n",
       "      <td>0.0211</td>\n",
       "      <td>1.3486</td>\n",
       "      <td>1.3274</td>\n",
       "      <td>0.0105</td>\n",
       "      <td>96.1517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1024</td>\n",
       "      <td>238.2891</td>\n",
       "      <td>0.0388</td>\n",
       "      <td>1.2679</td>\n",
       "      <td>1.2291</td>\n",
       "      <td>0.0097</td>\n",
       "      <td>103.7283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1536</td>\n",
       "      <td>358.3906</td>\n",
       "      <td>0.0594</td>\n",
       "      <td>1.2990</td>\n",
       "      <td>1.2396</td>\n",
       "      <td>0.0098</td>\n",
       "      <td>102.7924</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   L (вход)  VRAM (MB)  TTFT (prefill, c)  E2E (c)  Decode (c)  \\\n",
       "0       512   128.2168             0.0211   1.3486      1.3274   \n",
       "1      1024   238.2891             0.0388   1.2679      1.2291   \n",
       "2      1536   358.3906             0.0594   1.2990      1.2396   \n",
       "\n",
       "   Decode per token (c)  TPS (ток/с)  \n",
       "0                0.0105      96.1517  \n",
       "1                0.0097     103.7283  \n",
       "2                0.0098     102.7924  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(df_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "task2-interpretation-final-correct",
   "metadata": {},
   "source": [
    "### Интерпретация результатов (фактические наблюдения)\n",
    "\n",
    "По текущим измерениям (L = 512/1024/1536, max_new_tokens = 128) видны следующие тренды:\n",
    "\n",
    "1. **VRAM** растет почти линейно с увеличением `L` — это ожидаемо из-за роста KV-кэша.\n",
    "2. **TTFT / prefill** монотонно растет с `L`, что соответствует росту стоимости внимания на полном контексте.\n",
    "3. **Decode time** и **TPS** почти не меняются (колебания в пределах шума). Это означает, что в данном диапазоне длин и для этой модели вклад внимания в шаге decode относительно мал по сравнению с постоянными затратами (MLP, argmax, оверхед запуска).\n",
    "4. **E2E** меняется слабо и в основном определяется суммой почти постоянного decode и умеренно растущего prefill.\n",
    "\n",
    "Итог: теоретически decode должен расти с `L`, но для небольшой модели и выбранных параметров эффект оказывается слабым и теряется в шуме. Чтобы усилить тренд, нужны большее `L`, больше `max_new_tokens` и/или более крупная модель.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "521f53d2",
   "metadata": {},
   "source": [
    "# Задание 3: Профилирование модели (PyTorch Profiler + TensorBoard)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cb4ed231",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.profiler as profiler\n",
    "from torch.profiler import ProfilerActivity, profile, record_function\n",
    "\n",
    "LOGDIR = \"./tb_profiler\"\n",
    "os.makedirs(LOGDIR, exist_ok=True)\n",
    "\n",
    "base_prompt = \"Искусственный интеллект и машинное обучение революционизировали современные технологии. \"\n",
    "\n",
    "\n",
    "def build_inputs(target_len):\n",
    "    base_tokens = tokenizer.encode(base_prompt)\n",
    "    repeat_count = max(1, target_len // max(1, len(base_tokens)))\n",
    "    prompt = base_prompt * repeat_count\n",
    "    return tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=target_len).to(DEVICE)\n",
    "\n",
    "\n",
    "@torch.inference_mode()\n",
    "def run_prefill_decode(inputs, decode_steps):\n",
    "    with record_function(\"prefill\"):\n",
    "        out = model(**inputs, use_cache=True)\n",
    "    past = out.past_key_values\n",
    "    next_token = out.logits[:, -1, :].argmax(dim=-1, keepdim=True)\n",
    "\n",
    "    with record_function(\"decode\"):\n",
    "        for _ in range(decode_steps):\n",
    "            out = model(input_ids=next_token, past_key_values=past, use_cache=True)\n",
    "            past = out.past_key_values\n",
    "            next_token = out.logits[:, -1, :].argmax(dim=-1, keepdim=True)\n",
    "\n",
    "\n",
    "def profile_scenario(name, inputs, decode_steps):\n",
    "    activities = [ProfilerActivity.CPU]\n",
    "    if torch.cuda.is_available():\n",
    "        activities.append(ProfilerActivity.CUDA)\n",
    "\n",
    "    with profile(\n",
    "        activities=activities,\n",
    "        record_shapes=True,\n",
    "        profile_memory=True,\n",
    "        with_stack=True,\n",
    "        on_trace_ready=profiler.tensorboard_trace_handler(os.path.join(LOGDIR, name)),\n",
    "    ) as prof:\n",
    "        run_prefill_decode(inputs, decode_steps)\n",
    "\n",
    "    return prof\n",
    "\n",
    "\n",
    "def top_ops_by_time(prof, top_n=5):\n",
    "    def pick_attr(event, names, default=None):\n",
    "        for name in names:\n",
    "            try:\n",
    "                value = getattr(event, name)\n",
    "            except AttributeError:\n",
    "                continue\n",
    "            if value is not None:\n",
    "                return value\n",
    "        return default\n",
    "\n",
    "    def has_attr(event, names):\n",
    "        for name in names:\n",
    "            try:\n",
    "                getattr(event, name)\n",
    "            except AttributeError:\n",
    "                continue\n",
    "            else:\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "    events = prof.key_averages()\n",
    "\n",
    "    # Prefer device_time to avoid deprecated cuda_time.\n",
    "    use_device_time = any(has_attr(e, [\"device_time_total\", \"device_time\"]) for e in events)\n",
    "\n",
    "    if use_device_time:\n",
    "        get_time = lambda e: pick_attr(e, [\"device_time_total\", \"device_time\"], 0.0)\n",
    "        get_self = lambda e: pick_attr(e, [\"self_device_time_total\", \"self_device_time\"], 0.0)\n",
    "        time_label = \"device_time_ms\"\n",
    "        self_label = \"self_device_time_ms\"\n",
    "    else:\n",
    "        get_time = lambda e: pick_attr(e, [\"cpu_time_total\", \"cpu_time\"], 0.0)\n",
    "        get_self = lambda e: pick_attr(e, [\"self_cpu_time_total\", \"self_cpu_time\"], 0.0)\n",
    "        time_label = \"cpu_time_ms\"\n",
    "        self_label = \"self_cpu_time_ms\"\n",
    "\n",
    "    total = sum(get_time(e) for e in events)\n",
    "\n",
    "    rows = []\n",
    "    for e in sorted(events, key=get_time, reverse=True)[:top_n]:\n",
    "        rows.append(\n",
    "            {\n",
    "                \"op\": e.key,\n",
    "                time_label: get_time(e) / 1000,\n",
    "                self_label: get_self(e) / 1000,\n",
    "                \"calls\": e.count,\n",
    "                \"share_%\": (get_time(e) / total * 100) if total else 0.0,\n",
    "            }\n",
    "        )\n",
    "    return pd.DataFrame(rows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e1eb10f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "op",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "device_time_ms",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "self_device_time_ms",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "calls",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "share_%",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "ref": "ec802ca4-9147-4ff1-ae38-05c41b861e2f",
       "rows": [
        [
         "0",
         "decode",
         "178.787325",
         "178.787325",
         "1",
         "34.06372366539119"
        ],
        [
         "1",
         "prefill",
         "61.007864",
         "61.007864",
         "1",
         "11.623614933059526"
        ],
        [
         "2",
         "prefill",
         "37.918571999999934",
         "0.0",
         "1",
         "7.22449289061312"
        ],
        [
         "3",
         "decode",
         "34.26454299999993",
         "0.0",
         "1",
         "6.528303526398818"
        ],
        [
         "4",
         "aten::addmm",
         "25.735496999999853",
         "25.735496999999853",
         "432",
         "4.903294225133123"
        ]
       ],
       "shape": {
        "columns": 5,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>op</th>\n",
       "      <th>device_time_ms</th>\n",
       "      <th>self_device_time_ms</th>\n",
       "      <th>calls</th>\n",
       "      <th>share_%</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>decode</td>\n",
       "      <td>178.787325</td>\n",
       "      <td>178.787325</td>\n",
       "      <td>1</td>\n",
       "      <td>34.063724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>prefill</td>\n",
       "      <td>61.007864</td>\n",
       "      <td>61.007864</td>\n",
       "      <td>1</td>\n",
       "      <td>11.623615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>prefill</td>\n",
       "      <td>37.918572</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>7.224493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>decode</td>\n",
       "      <td>34.264543</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>6.528304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>aten::addmm</td>\n",
       "      <td>25.735497</td>\n",
       "      <td>25.735497</td>\n",
       "      <td>432</td>\n",
       "      <td>4.903294</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            op  device_time_ms  self_device_time_ms  calls    share_%\n",
       "0       decode      178.787325           178.787325      1  34.063724\n",
       "1      prefill       61.007864            61.007864      1  11.623615\n",
       "2      prefill       37.918572             0.000000      1   7.224493\n",
       "3       decode       34.264543             0.000000      1   6.528304\n",
       "4  aten::addmm       25.735497            25.735497    432   4.903294"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "op",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "device_time_ms",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "self_device_time_ms",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "calls",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "share_%",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "ref": "af20a111-79e5-4ea9-91b1-bbe558eb101a",
       "rows": [
        [
         "0",
         "decode",
         "2303.8294000000005",
         "2303.8294000000005",
         "1",
         "58.52876529817006"
        ],
        [
         "1",
         "decode",
         "435.11681400000066",
         "0.0",
         "1",
         "11.0541387673469"
        ],
        [
         "2",
         "aten::addmm",
         "216.24939799998333",
         "216.2265259999836",
         "6192",
         "5.493814021737716"
        ],
        [
         "3",
         "std::enable_if<!(false), void>::type internal::gemvx::kernel<int, int, float, float, float, float, false, true, false, false, 6, false, cublasGemvParamsEx<int, cublasGemvTensorStridedBatched<float const>, cublasGemvTensorStridedBatched<float const>, cublasGemvTensorStridedBatched<float>, float> >(cublasGemvParamsEx<int, cublasGemvTensorStridedBatched<float const>, cublasGemvTensorStridedBatched<float const>, cublasGemvTensorStridedBatched<float>, float>)",
         "79.99270799999339",
         "79.99270799999339",
         "3072",
         "2.0322140311676997"
        ],
        [
         "4",
         "aten::scaled_dot_product_attention",
         "70.6079649999985",
         "0.0",
         "1548",
         "1.7937947192037385"
        ]
       ],
       "shape": {
        "columns": 5,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>op</th>\n",
       "      <th>device_time_ms</th>\n",
       "      <th>self_device_time_ms</th>\n",
       "      <th>calls</th>\n",
       "      <th>share_%</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>decode</td>\n",
       "      <td>2303.829400</td>\n",
       "      <td>2303.829400</td>\n",
       "      <td>1</td>\n",
       "      <td>58.528765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>decode</td>\n",
       "      <td>435.116814</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>11.054139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>aten::addmm</td>\n",
       "      <td>216.249398</td>\n",
       "      <td>216.226526</td>\n",
       "      <td>6192</td>\n",
       "      <td>5.493814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>std::enable_if&lt;!(false), void&gt;::type internal:...</td>\n",
       "      <td>79.992708</td>\n",
       "      <td>79.992708</td>\n",
       "      <td>3072</td>\n",
       "      <td>2.032214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>aten::scaled_dot_product_attention</td>\n",
       "      <td>70.607965</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1548</td>\n",
       "      <td>1.793795</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  op  device_time_ms  \\\n",
       "0                                             decode     2303.829400   \n",
       "1                                             decode      435.116814   \n",
       "2                                        aten::addmm      216.249398   \n",
       "3  std::enable_if<!(false), void>::type internal:...       79.992708   \n",
       "4                 aten::scaled_dot_product_attention       70.607965   \n",
       "\n",
       "   self_device_time_ms  calls    share_%  \n",
       "0          2303.829400      1  58.528765  \n",
       "1             0.000000      1  11.054139  \n",
       "2           216.226526   6192   5.493814  \n",
       "3            79.992708   3072   2.032214  \n",
       "4             0.000000   1548   1.793795  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Сценарий 1: prefill-доминантный (длинный вход, короткая генерация)\n",
    "prefill_inputs = build_inputs(1024)\n",
    "prof_prefill = profile_scenario(\"prefill_dominant\", prefill_inputs, decode_steps=8)\n",
    "top_prefill = top_ops_by_time(prof_prefill)\n",
    "display(top_prefill)\n",
    "\n",
    "# Сценарий 2: decode-доминантный (короткий вход, длинная генерация)\n",
    "decode_inputs = build_inputs(16)\n",
    "prof_decode = profile_scenario(\"decode_dominant\", decode_inputs, decode_steps=128)\n",
    "top_decode = top_ops_by_time(prof_decode)\n",
    "display(top_decode)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cafc4e33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-f4ac4414f95f4535\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-f4ac4414f95f4535\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir ./tb_profiler\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3921d849",
   "metadata": {},
   "source": [
    "### Интерпретация профилирования\n",
    "\n",
    "**Сценарий prefill-dominant (L≈1024, decode_steps=8), top-5 CUDA:**\n",
    "- decode — 176.37 ms (51.04%)\n",
    "- prefill — 83.72 ms (24.23%)\n",
    "- prefill — 37.82 ms (10.94%)\n",
    "- decode — 34.90 ms (10.10%)\n",
    "- ampere_sgemm_128x64_tn — 6.94 ms (2.01%)\n",
    "\n",
    "**Сценарий decode-dominant (L≈16, decode_steps=128), top-5 CUDA:**\n",
    "- decode — 2318.71 ms (80.40%)\n",
    "- decode — 533.76 ms (18.51%)\n",
    "- prefill — 21.70 ms (0.75%)\n",
    "- prefill — 5.72 ms (0.20%)\n",
    "- ampere_sgemm_64x32_sliced1x4_tn — 0.88 ms (0.03%)\n",
    "\n",
    "**Выводы по логам:**\n",
    "1. Большая часть времени находится внутри регионов `prefill`/`decode`, а отдельные GEMM-ядра дают малую долю. Это означает, что время распределено по множеству мелких операций, а не одной крупной матрице. Явных HtoD/DtoH копирований в top-5 нет.\n",
    "2. По этой картине сценарии ближе к memory/overhead-bound (много мелких ops и запусков), особенно на decode. Для compute-bound ожидался бы доминирующий вклад крупных GEMM/attention.\n",
    "3. Prefill vs decode: в decode-dominant режиме почти все время уходит на decode (~99%). В prefill-dominant режиме decode все равно занимает заметную долю (~61%), что объясняется несколькими шагами decode и небольшим batch.\n",
    "4. Релевантные оптимизации: для prefill — FlashAttention/SDPA и увеличение batch/sequence; для decode — оптимизация KV-кэша (paged KV), kernel fusion (attention/MLP/layernorm), speculative decoding или батчинг запросов, чтобы снизить per-token overhead.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ff21d06",
   "metadata": {},
   "source": [
    "# Задание 4: Эксперименты с квантизованной моделью (4-bit)\n",
    "\n",
    "> Примечание: перед запуском задания 4 надо перезапустить ядро интерпретатора и выполнить все ячейки до конца раздела 1. Это очистит память и обеспечит корректные замеры.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "81fbcded",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "precision",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "vram_allocated_mb",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "vram_reserved_mb",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "ref": "66672771-a9b8-4000-9bf9-d8084a7ea50e",
       "rows": [
        [
         "0",
         "fp16",
         "291.7373046875",
         "644.0"
        ],
        [
         "1",
         "4-bit",
         "462.3740234375",
         "586.0"
        ]
       ],
       "shape": {
        "columns": 3,
        "rows": 2
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>precision</th>\n",
       "      <th>vram_allocated_mb</th>\n",
       "      <th>vram_reserved_mb</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>fp16</td>\n",
       "      <td>291.737305</td>\n",
       "      <td>644.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4-bit</td>\n",
       "      <td>462.374023</td>\n",
       "      <td>586.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  precision  vram_allocated_mb  vram_reserved_mb\n",
       "0      fp16         291.737305             644.0\n",
       "1     4-bit         462.374023             586.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import gc\n",
    "from transformers import BitsAndBytesConfig\n",
    "\n",
    "\n",
    "def cleanup_model(m):\n",
    "    del m\n",
    "    gc.collect()\n",
    "    if DEVICE == \"cuda\":\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.reset_peak_memory_stats()\n",
    "        torch.cuda.synchronize()\n",
    "\n",
    "\n",
    "def get_vram_stats_mb():\n",
    "    if DEVICE != \"cuda\":\n",
    "        return {\"vram_allocated_mb\": 0.0, \"vram_reserved_mb\": 0.0}\n",
    "    torch.cuda.synchronize()\n",
    "    return {\n",
    "        \"vram_allocated_mb\": torch.cuda.memory_allocated() / (1024**2),\n",
    "        \"vram_reserved_mb\": torch.cuda.memory_reserved() / (1024**2),\n",
    "    }\n",
    "\n",
    "\n",
    "def load_tokenizer_safe(model_name, local_path):\n",
    "    if \"tokenizer\" in globals():\n",
    "        return tokenizer\n",
    "    local_only = os.path.isdir(local_path)\n",
    "    src = local_path if local_only else model_name\n",
    "    kwargs = {\"local_files_only\": True} if local_only else {}\n",
    "    return AutoTokenizer.from_pretrained(src, **kwargs)\n",
    "\n",
    "\n",
    "def load_fp16_model(model_name, local_path):\n",
    "    if DEVICE != \"cuda\":\n",
    "        raise RuntimeError(\"FP16/4-bit эксперименты требуют CUDA.\")\n",
    "\n",
    "    local_only = os.path.isdir(local_path)\n",
    "    src = local_path if local_only else model_name\n",
    "    kwargs = {\"local_files_only\": True} if local_only else {}\n",
    "\n",
    "    model_fp16 = AutoModelForCausalLM.from_pretrained(\n",
    "        src,\n",
    "        dtype=torch.float16,\n",
    "        device_map=\"auto\",\n",
    "        **kwargs,\n",
    "    )\n",
    "    model_fp16.eval()\n",
    "    model_fp16.config.use_cache = True\n",
    "    return model_fp16\n",
    "\n",
    "\n",
    "def load_4bit_model(model_name, local_path):\n",
    "    if DEVICE != \"cuda\":\n",
    "        raise RuntimeError(\"4-bit эксперименты требуют CUDA.\")\n",
    "\n",
    "    local_only = os.path.isdir(local_path)\n",
    "    src = local_path if local_only else model_name\n",
    "    kwargs = {\"local_files_only\": True} if local_only else {}\n",
    "\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "        bnb_4bit_compute_dtype=torch.float16,\n",
    "    )\n",
    "\n",
    "    model_4bit = AutoModelForCausalLM.from_pretrained(\n",
    "        src,\n",
    "        quantization_config=bnb_config,\n",
    "        device_map=\"auto\",\n",
    "        **kwargs,\n",
    "    )\n",
    "    model_4bit.eval()\n",
    "    model_4bit.config.use_cache = True\n",
    "    return model_4bit\n",
    "\n",
    "\n",
    "if \"MODEL_NAME\" not in globals() or \"LOCAL_MODEL_PATH\" not in globals():\n",
    "    raise RuntimeError(\"Сначала выполните раздел 1 (MODEL_NAME/LOCAL_MODEL_PATH).\")\n",
    "\n",
    "tokenizer = load_tokenizer_safe(MODEL_NAME, LOCAL_MODEL_PATH)\n",
    "\n",
    "for name in (\"model\", \"model_fp16\", \"model_4bit\"):\n",
    "    if name in globals():\n",
    "        cleanup_model(globals()[name])\n",
    "        globals().pop(name, None)\n",
    "\n",
    "model_fp16 = load_fp16_model(MODEL_NAME, LOCAL_MODEL_PATH)\n",
    "vram_fp16 = get_vram_stats_mb()\n",
    "cleanup_model(model_fp16)\n",
    "\n",
    "model_4bit = load_4bit_model(MODEL_NAME, LOCAL_MODEL_PATH)\n",
    "vram_4bit = get_vram_stats_mb()\n",
    "\n",
    "df_vram_compare = pd.DataFrame(\n",
    "    [\n",
    "        {\"precision\": \"fp16\", **vram_fp16},\n",
    "        {\"precision\": \"4-bit\", **vram_4bit},\n",
    "    ]\n",
    ")\n",
    "display(df_vram_compare)\n",
    "\n",
    "if tokenizer.pad_token_id is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    model_4bit.config.pad_token_id = model_4bit.config.eos_token_id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bc82f50a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Прогрев GPU (4-bit)...\n",
      "Прогрев завершен.\n",
      "\n",
      "--- Измерения для L = 512 (4-bit, усреднение по 10 итерациям) ---\n",
      "Результаты для L=512 сохранены.\n",
      "\n",
      "--- Измерения для L = 1024 (4-bit, усреднение по 10 итерациям) ---\n",
      "Результаты для L=1024 сохранены.\n",
      "\n",
      "--- Измерения для L = 1536 (4-bit, усреднение по 10 итерациям) ---\n",
      "Результаты для L=1536 сохранены.\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "L (вход)",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "VRAM (MB)",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "TTFT (prefill, c)",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "E2E (c)",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Decode (c)",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Decode per token (c)",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "TPS (ток/с)",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "ref": "b2f0bf75-2e99-40a1-a621-b3282efb9a2c",
       "rows": [
        [
         "0",
         "512",
         "59.668",
         "0.0297",
         "2.0459",
         "2.0162",
         "0.0159",
         "63.0259"
        ],
        [
         "1",
         "1024",
         "120.8652",
         "0.0406",
         "2.1359",
         "2.0953",
         "0.0165",
         "60.6804"
        ],
        [
         "2",
         "1536",
         "179.3384",
         "0.048",
         "2.0892",
         "2.0411",
         "0.0161",
         "62.245"
        ]
       ],
       "shape": {
        "columns": 7,
        "rows": 3
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>L (вход)</th>\n",
       "      <th>VRAM (MB)</th>\n",
       "      <th>TTFT (prefill, c)</th>\n",
       "      <th>E2E (c)</th>\n",
       "      <th>Decode (c)</th>\n",
       "      <th>Decode per token (c)</th>\n",
       "      <th>TPS (ток/с)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>512</td>\n",
       "      <td>59.6680</td>\n",
       "      <td>0.0297</td>\n",
       "      <td>2.0459</td>\n",
       "      <td>2.0162</td>\n",
       "      <td>0.0159</td>\n",
       "      <td>63.0259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1024</td>\n",
       "      <td>120.8652</td>\n",
       "      <td>0.0406</td>\n",
       "      <td>2.1359</td>\n",
       "      <td>2.0953</td>\n",
       "      <td>0.0165</td>\n",
       "      <td>60.6804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1536</td>\n",
       "      <td>179.3384</td>\n",
       "      <td>0.0480</td>\n",
       "      <td>2.0892</td>\n",
       "      <td>2.0411</td>\n",
       "      <td>0.0161</td>\n",
       "      <td>62.2450</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   L (вход)  VRAM (MB)  TTFT (prefill, c)  E2E (c)  Decode (c)  \\\n",
       "0       512    59.6680             0.0297   2.0459      2.0162   \n",
       "1      1024   120.8652             0.0406   2.1359      2.0953   \n",
       "2      1536   179.3384             0.0480   2.0892      2.0411   \n",
       "\n",
       "   Decode per token (c)  TPS (ток/с)  \n",
       "0                0.0159      63.0259  \n",
       "1                0.0165      60.6804  \n",
       "2                0.0161      62.2450  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@torch.inference_mode()\n",
    "def measure_performance_with_model(model, input_ids, max_new_tokens):\n",
    "    if DEVICE == \"cuda\":\n",
    "        torch.cuda.synchronize()\n",
    "    t0 = time.perf_counter()\n",
    "\n",
    "    out = model(input_ids=input_ids, use_cache=True)\n",
    "\n",
    "    if DEVICE == \"cuda\":\n",
    "        torch.cuda.synchronize()\n",
    "    prefill_time = time.perf_counter() - t0\n",
    "\n",
    "    past = out.past_key_values\n",
    "    next_token = out.logits[:, -1, :].argmax(dim=-1, keepdim=True)\n",
    "\n",
    "    decode_tokens = max(max_new_tokens - 1, 0)\n",
    "\n",
    "    if DEVICE == \"cuda\":\n",
    "        torch.cuda.synchronize()\n",
    "    t1 = time.perf_counter()\n",
    "\n",
    "    for _ in range(decode_tokens):\n",
    "        out = model(input_ids=next_token, past_key_values=past, use_cache=True)\n",
    "        past = out.past_key_values\n",
    "        next_token = out.logits[:, -1, :].argmax(dim=-1, keepdim=True)\n",
    "\n",
    "    if DEVICE == \"cuda\":\n",
    "        torch.cuda.synchronize()\n",
    "    decode_time = time.perf_counter() - t1\n",
    "    decode_time_per_token = decode_time / decode_tokens if decode_tokens > 0 else 0\n",
    "\n",
    "    e2e_time = prefill_time + decode_time\n",
    "    tps = decode_tokens / decode_time if decode_time > 0 else 0\n",
    "\n",
    "    return {\n",
    "        \"TTFT (prefill, c)\": prefill_time,\n",
    "        \"Decode (c)\": decode_time,\n",
    "        \"Decode per token (c)\": decode_time_per_token,\n",
    "        \"E2E (c)\": e2e_time,\n",
    "        \"TPS (ток/с)\": tps,\n",
    "    }\n",
    "\n",
    "\n",
    "L_values_4bit = [512, 1024, 1536]\n",
    "max_new_tokens_4bit = 128\n",
    "num_iterations_4bit = 10\n",
    "all_results_4bit = []\n",
    "\n",
    "base_prompt = \"Искусственный интеллект и машинное обучение революционизировали современные технологии. \"\n",
    "base_tokens = tokenizer.encode(base_prompt)\n",
    "\n",
    "if DEVICE == \"cuda\":\n",
    "    print(\"Прогрев GPU (4-bit)...\")\n",
    "    warm_inputs = tokenizer(base_prompt, return_tensors=\"pt\").to(DEVICE)\n",
    "    _ = measure_performance_with_model(model_4bit, warm_inputs[\"input_ids\"], 16)\n",
    "    print(\"Прогрев завершен.\")\n",
    "\n",
    "for L in L_values_4bit:\n",
    "    print(f\"\\n--- Измерения для L = {L} (4-bit, усреднение по {num_iterations_4bit} итерациям) ---\")\n",
    "    iter_results = []\n",
    "\n",
    "    repeat_count = max(1, L // max(1, len(base_tokens)))\n",
    "    prompt = base_prompt * repeat_count\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=L).to(DEVICE)\n",
    "    input_ids = inputs[\"input_ids\"]\n",
    "\n",
    "    for _ in range(num_iterations_4bit):\n",
    "        if DEVICE == \"cuda\":\n",
    "            torch.cuda.reset_peak_memory_stats()\n",
    "            base_alloc = torch.cuda.memory_allocated()\n",
    "\n",
    "        perf_metrics = measure_performance_with_model(model_4bit, input_ids, max_new_tokens_4bit)\n",
    "\n",
    "        if DEVICE == \"cuda\":\n",
    "            peak_alloc = torch.cuda.max_memory_allocated()\n",
    "            perf_metrics[\"VRAM (MB)\"] = (peak_alloc - base_alloc) / (1024**2)\n",
    "        else:\n",
    "            perf_metrics[\"VRAM (MB)\"] = 0\n",
    "\n",
    "        iter_results.append(perf_metrics)\n",
    "\n",
    "    avg_results = pd.DataFrame(iter_results).mean().to_dict()\n",
    "    avg_results[\"L (вход)\"] = L\n",
    "    all_results_4bit.append(avg_results)\n",
    "    print(f\"Результаты для L={L} сохранены.\")\n",
    "\n",
    "df_results_4bit = pd.DataFrame(all_results_4bit)\n",
    "df_results_4bit = df_results_4bit[\n",
    "    [\"L (вход)\", \"VRAM (MB)\", \"TTFT (prefill, c)\", \"E2E (c)\", \"Decode (c)\", \"Decode per token (c)\", \"TPS (ток/с)\"]\n",
    "]\n",
    "df_results_4bit = df_results_4bit.round(4)\n",
    "display(df_results_4bit)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c6cf366",
   "metadata": {},
   "source": [
    "### Интерпретация 4-bit результатов\n",
    "\n",
    "1. **VRAM (веса модели)**: по `df_vram_compare` получено FP16 ≈ 291.74 MB allocated и 644.0 MB reserved, а 4-bit ≈ 462.37 MB allocated и 586.0 MB reserved. Это не «чистый размер весов», а состояние CUDA‑аллокатора с буферами и метаданными квантования, поэтому значения читаем аккуратно.\n",
    "2. **VRAM (динамика во время инференса)**: пиковая прибавка памяти на KV‑кэш заметно ниже в 4-bit: ~59.7 / 120.9 / 179.3 MB против ~128.2 / 238.3 / 358.4 MB в FP16 (примерно в 2 раза меньше на всех L).\n",
    "3. **TTFT/Decode/TPS**: TTFT сопоставим по порядку (≈0.03–0.05 c), но decode и E2E в 4-bit ощутимо медленнее (≈2.04–2.14 c против ≈1.27–1.35 c), TPS ниже (≈60–63 ток/с против ≈96–104 ток/с).\n",
    "4. **Prefill vs decode**: в обеих версиях доминирует decode, но в 4-bit он еще сильнее определяет итоговое E2E‑время.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ebcf8cb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "op",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "device_time_ms",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "self_device_time_ms",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "calls",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "share_%",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "ref": "f6d83c06-c578-4580-a2f6-47a4fc354522",
       "rows": [
        [
         "0",
         "decode",
         "318.12686699999995",
         "318.12686699999995",
         "1",
         "50.190186440685544"
        ],
        [
         "1",
         "prefill",
         "60.935773999999995",
         "60.935773999999995",
         "1",
         "9.613705019034054"
        ],
        [
         "2",
         "prefill",
         "34.91407400000002",
         "0.0",
         "1",
         "5.5083177978296725"
        ],
        [
         "3",
         "aten::linear",
         "26.412398999999965",
         "0.0",
         "57",
         "4.167026955808089"
        ],
        [
         "4",
         "decode",
         "26.150153999998846",
         "0.0",
         "1",
         "4.125653130430449"
        ]
       ],
       "shape": {
        "columns": 5,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>op</th>\n",
       "      <th>device_time_ms</th>\n",
       "      <th>self_device_time_ms</th>\n",
       "      <th>calls</th>\n",
       "      <th>share_%</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>decode</td>\n",
       "      <td>318.126867</td>\n",
       "      <td>318.126867</td>\n",
       "      <td>1</td>\n",
       "      <td>50.190186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>prefill</td>\n",
       "      <td>60.935774</td>\n",
       "      <td>60.935774</td>\n",
       "      <td>1</td>\n",
       "      <td>9.613705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>prefill</td>\n",
       "      <td>34.914074</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>5.508318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>aten::linear</td>\n",
       "      <td>26.412399</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>57</td>\n",
       "      <td>4.167027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>decode</td>\n",
       "      <td>26.150154</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>4.125653</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             op  device_time_ms  self_device_time_ms  calls    share_%\n",
       "0        decode      318.126867           318.126867      1  50.190186\n",
       "1       prefill       60.935774            60.935774      1   9.613705\n",
       "2       prefill       34.914074             0.000000      1   5.508318\n",
       "3  aten::linear       26.412399             0.000000     57   4.167027\n",
       "4        decode       26.150154             0.000000      1   4.125653"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "op",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "device_time_ms",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "self_device_time_ms",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "calls",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "share_%",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "ref": "c929797a-f1b2-45ce-86a4-847b98fff179",
       "rows": [
        [
         "0",
         "decode",
         "5084.9386890000005",
         "5084.9386890000005",
         "1",
         "55.352779214665325"
        ],
        [
         "1",
         "decode",
         "1003.2001730001266",
         "0.0",
         "1",
         "10.92046946491513"
        ],
        [
         "2",
         "aten::linear",
         "348.9835180000014",
         "0.0",
         "177",
         "3.798906693447305"
        ],
        [
         "3",
         "aten::matmul",
         "329.2142990000014",
         "0.0",
         "129",
         "3.583706219758099"
        ],
        [
         "4",
         "aten::mm",
         "329.2142990000014",
         "329.2142990000014",
         "129",
         "3.583706219758099"
        ]
       ],
       "shape": {
        "columns": 5,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>op</th>\n",
       "      <th>device_time_ms</th>\n",
       "      <th>self_device_time_ms</th>\n",
       "      <th>calls</th>\n",
       "      <th>share_%</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>decode</td>\n",
       "      <td>5084.938689</td>\n",
       "      <td>5084.938689</td>\n",
       "      <td>1</td>\n",
       "      <td>55.352779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>decode</td>\n",
       "      <td>1003.200173</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>10.920469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>aten::linear</td>\n",
       "      <td>348.983518</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>177</td>\n",
       "      <td>3.798907</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>aten::matmul</td>\n",
       "      <td>329.214299</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>129</td>\n",
       "      <td>3.583706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>aten::mm</td>\n",
       "      <td>329.214299</td>\n",
       "      <td>329.214299</td>\n",
       "      <td>129</td>\n",
       "      <td>3.583706</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             op  device_time_ms  self_device_time_ms  calls    share_%\n",
       "0        decode     5084.938689          5084.938689      1  55.352779\n",
       "1        decode     1003.200173             0.000000      1  10.920469\n",
       "2  aten::linear      348.983518             0.000000    177   3.798907\n",
       "3  aten::matmul      329.214299             0.000000    129   3.583706\n",
       "4      aten::mm      329.214299           329.214299    129   3.583706"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "LOGDIR_4BIT = \"./tb_profiler_4bit\"\n",
    "os.makedirs(LOGDIR_4BIT, exist_ok=True)\n",
    "\n",
    "\n",
    "def build_inputs_4bit(target_len, prompt_text):\n",
    "    base_tokens = tokenizer.encode(prompt_text)\n",
    "    repeat_count = max(1, target_len // max(1, len(base_tokens)))\n",
    "    prompt = prompt_text * repeat_count\n",
    "    return tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=target_len).to(DEVICE)\n",
    "\n",
    "\n",
    "@torch.inference_mode()\n",
    "def run_prefill_decode_4bit(model, inputs, decode_steps):\n",
    "    with record_function(\"prefill\"):\n",
    "        out = model(**inputs, use_cache=True)\n",
    "    past = out.past_key_values\n",
    "    next_token = out.logits[:, -1, :].argmax(dim=-1, keepdim=True)\n",
    "\n",
    "    with record_function(\"decode\"):\n",
    "        for _ in range(decode_steps):\n",
    "            out = model(input_ids=next_token, past_key_values=past, use_cache=True)\n",
    "            past = out.past_key_values\n",
    "            next_token = out.logits[:, -1, :].argmax(dim=-1, keepdim=True)\n",
    "\n",
    "\n",
    "def profile_scenario_4bit(name, model, inputs, decode_steps, logdir):\n",
    "    activities = [ProfilerActivity.CPU]\n",
    "    if torch.cuda.is_available():\n",
    "        activities.append(ProfilerActivity.CUDA)\n",
    "\n",
    "    with profile(\n",
    "        activities=activities,\n",
    "        record_shapes=True,\n",
    "        profile_memory=True,\n",
    "        with_stack=True,\n",
    "        on_trace_ready=profiler.tensorboard_trace_handler(os.path.join(logdir, name)),\n",
    "    ) as prof:\n",
    "        run_prefill_decode_4bit(model, inputs, decode_steps)\n",
    "\n",
    "    return prof\n",
    "\n",
    "\n",
    "def top_ops_by_time_4bit(prof, top_n=5):\n",
    "    def pick_attr(event, names, default=None):\n",
    "        for name in names:\n",
    "            try:\n",
    "                value = getattr(event, name)\n",
    "            except AttributeError:\n",
    "                continue\n",
    "            if value is not None:\n",
    "                return value\n",
    "        return default\n",
    "\n",
    "    def has_attr(event, names):\n",
    "        for name in names:\n",
    "            try:\n",
    "                getattr(event, name)\n",
    "            except AttributeError:\n",
    "                continue\n",
    "            else:\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "    events = prof.key_averages()\n",
    "\n",
    "    use_device_time = any(has_attr(e, [\"device_time_total\", \"device_time\"]) for e in events)\n",
    "\n",
    "    if use_device_time:\n",
    "        get_time = lambda e: pick_attr(e, [\"device_time_total\", \"device_time\"], 0.0)\n",
    "        get_self = lambda e: pick_attr(e, [\"self_device_time_total\", \"self_device_time\"], 0.0)\n",
    "        time_label = \"device_time_ms\"\n",
    "        self_label = \"self_device_time_ms\"\n",
    "    else:\n",
    "        get_time = lambda e: pick_attr(e, [\"cpu_time_total\", \"cpu_time\"], 0.0)\n",
    "        get_self = lambda e: pick_attr(e, [\"self_cpu_time_total\", \"self_cpu_time\"], 0.0)\n",
    "        time_label = \"cpu_time_ms\"\n",
    "        self_label = \"self_cpu_time_ms\"\n",
    "\n",
    "    total = sum(get_time(e) for e in events)\n",
    "\n",
    "    rows = []\n",
    "    for e in sorted(events, key=get_time, reverse=True)[:top_n]:\n",
    "        rows.append(\n",
    "            {\n",
    "                \"op\": e.key,\n",
    "                time_label: get_time(e) / 1000,\n",
    "                self_label: get_self(e) / 1000,\n",
    "                \"calls\": e.count,\n",
    "                \"share_%\": (get_time(e) / total * 100) if total else 0.0,\n",
    "            }\n",
    "        )\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "\n",
    "prefill_inputs_4bit = build_inputs_4bit(1024, base_prompt)\n",
    "prof_prefill_4bit = profile_scenario_4bit(\"prefill_dominant_4bit\", model_4bit, prefill_inputs_4bit, 8, LOGDIR_4BIT)\n",
    "top_prefill_4bit = top_ops_by_time_4bit(prof_prefill_4bit)\n",
    "display(top_prefill_4bit)\n",
    "\n",
    "decode_inputs_4bit = build_inputs_4bit(16, base_prompt)\n",
    "prof_decode_4bit = profile_scenario_4bit(\"decode_dominant_4bit\", model_4bit, decode_inputs_4bit, 128, LOGDIR_4BIT)\n",
    "top_decode_4bit = top_ops_by_time_4bit(prof_decode_4bit)\n",
    "display(top_decode_4bit)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d8be4e68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-44f051d96623b580\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-44f051d96623b580\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir ./tb_profiler_4bit\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9051b84a",
   "metadata": {},
   "source": [
    "### Интерпретация профилирования 4-bit\n",
    "\n",
    "**Prefill-dominant (L≈1024, decode_steps=8), top-5 device_time:**\n",
    "- decode — 318.13 ms (50.19%)\n",
    "- prefill — 60.94 ms (9.61%)\n",
    "- prefill — 34.91 ms (5.51%)\n",
    "- aten::linear — 26.41 ms (4.17%)\n",
    "- decode — 26.15 ms (4.13%)\n",
    "\n",
    "**Decode-dominant (L≈16, decode_steps=128), top-5 device_time:**\n",
    "- decode — 5084.94 ms (55.35%)\n",
    "- decode — 1003.20 ms (10.92%)\n",
    "- aten::linear — 348.98 ms (3.80%)\n",
    "- aten::matmul — 329.21 ms (3.58%)\n",
    "- aten::mm — 329.21 ms (3.58%)\n",
    "\n",
    "**Выводы:**\n",
    "1. В обоих сценариях лидирует `decode`, а в top‑5 заметны `aten::linear/matmul/mm`, что указывает на вклад матмул‑операций и де‑квантного оверхеда.\n",
    "2. В decode‑dominant режиме почти все время уходит на токен‑токен декодирование (две строки `decode` дают ≈66% суммарного времени).\n",
    "3. Явных HtoD/DtoH копирований в top‑5 нет — узкое место не в трансферах, а в вычислениях и оверхеде мелких ops.\n",
    "4. По сравнению с FP16 4-bit выглядит более «overhead/memory‑bound»: больше отдельных linear/matmul в топе и слабее выраженный вклад attention.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-course",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
